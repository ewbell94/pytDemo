{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3608376-c567-4e35-8a8a-05621819b8f6",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "## 1. Tensors\n",
    "PyTorch operates through manipulation of a data structure known as the \"tensor\".  Tensors are mathematically known as a high-dimensional data structure (i.e., 3 or more dimensional matrices), but in PyTorch, any dimensionality of array can be represented as a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d063ec-0b22-4c08-8180-0b98a1c71cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#All are welcome in the world of PyTorch tensors!\n",
    "print(torch.ones(1)) #A scalar?\n",
    "print(torch.ones(10)) #A vector?\n",
    "print(torch.ones(3,3)) #A matrix?\n",
    "print(torch.ones(3,2,1)) #A 3D tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141180e-f9f7-4803-9e97-6424f462f6f5",
   "metadata": {},
   "source": [
    "Tensors must contain only entries of the same data type, and the dimension of each axis must be consistent across the layers of the tensor (i.e. a tensor is an *array* of *arrays* not a *list* of *lists* like Python likes to do).  If you're familiar with numpy arrays, Tensors can be easily constructed from python lists and numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c560e5c7-bf98-4a5f-9281-06b252d440f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(torch.tensor([[i for i in range(10)] for j in range(10)]))\n",
    "print(torch.tensor(np.ones((10,10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490ea9-17f3-43b5-bbf8-405afb9909fa",
   "metadata": {},
   "source": [
    "You can easily create tensors of a given size using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230cef09-3046-449e-b9d9-ddfd51647281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.9906, 0.3294, 0.7882],\n",
      "        [0.9233, 0.6241, 0.4668],\n",
      "        [0.3117, 0.3975, 0.9637]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(3,3)) #Make a tensor of zeros\n",
    "print(torch.ones(3,3)) #Make a tensor of ones\n",
    "print(torch.rand(3,3)) #Make a tensor of random numbers between 0 and 1\n",
    "print(torch.empty(3,3)) #Make a tensor of uninitialized values (might be zeros, might be weird numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe42937-57c4-45d5-98fe-e5bd4716f891",
   "metadata": {},
   "source": [
    "A few attributes of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21de8ae-2daa-4e67-8791-5d0b7242160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5,5)\n",
    "\n",
    "print(a.size()) #Get the size of each\n",
    "print(a.dtype) #Get the data type of the \n",
    "print(a.device) #Get which device the tensor is on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c3790-e702-462d-b7db-db53cc4187f0",
   "metadata": {},
   "source": [
    "But hold on... what's that last one again?  What does it mean to have a tensor on a device?  The answer is that the main difference between numpy arrays and torch tensors is that tensors can be put on a GPU, thus allowing all mathematical operations involving it to be GPU accelerated!  For that to be able to happen, the following has to return true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d01a211-3479-4054-b5b5-a34fb210dc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f8448-8de9-44e9-92d0-71457e8b8a44",
   "metadata": {},
   "source": [
    "If this is true, congratulations!  You're ready to accelerate using the GPU!  If it's false, you either don't have a GPU, or don't have CUDA or PyTorch set up in a way that allows for GPU acceleration.  Getting this set up is incredibly system specific and beyond the scope of this tutorial.  However, you can take advantage of your GPU by moving your tensor to the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88d310f5-04d9-430f-8cf3-19153dd56b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    a.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1097f51-ae29-46fe-9638-eb31ca67d40d",
   "metadata": {},
   "source": [
    "Note that all created tensors will be stored on the CPU by default unless specified otherwise on initialization (i.e. specifying `device=\"cuda\"` when the tensor is made).  Also, operations between tensors require that the tensors be on the same device; you'll get an error otherwise.  So what are the operations can you do to tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c430f621-7831-4efd-9dbf-deec2123da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2558, 0.4919, 0.3648, 0.8303, 0.1401],\n",
      "        [0.2633, 0.1573, 0.1584, 0.9556, 0.5556],\n",
      "        [0.1617, 0.5476, 0.4154, 0.6731, 0.1571],\n",
      "        [0.8064, 0.8664, 0.0642, 0.0574, 0.3945],\n",
      "        [0.5852, 0.8945, 0.9340, 0.9404, 0.4619]])\n",
      "tensor([[0.4583, 0.1443, 0.2191, 0.3032, 0.6162],\n",
      "        [0.2973, 0.8497, 0.0771, 0.0942, 0.3936],\n",
      "        [0.6465, 0.1874, 0.3082, 0.3536, 0.2133],\n",
      "        [0.7136, 0.7007, 0.9473, 0.9918, 0.1927],\n",
      "        [0.6428, 0.7178, 0.4280, 0.6711, 0.3129]])\n",
      "tensor([[0.7141, 0.6362, 0.5839, 1.1335, 0.7563],\n",
      "        [0.5607, 1.0071, 0.2354, 1.0498, 0.9492],\n",
      "        [0.8082, 0.7350, 0.7237, 1.0267, 0.3704],\n",
      "        [1.5200, 1.5671, 1.0115, 1.0493, 0.5872],\n",
      "        [1.2280, 1.6123, 1.3620, 1.6115, 0.7748]])\n",
      "tensor([[0.1172, 0.0710, 0.0799, 0.2517, 0.0863],\n",
      "        [0.0783, 0.1337, 0.0122, 0.0900, 0.2187],\n",
      "        [0.1045, 0.1026, 0.1281, 0.2380, 0.0335],\n",
      "        [0.5754, 0.6071, 0.0608, 0.0570, 0.0760],\n",
      "        [0.3762, 0.6420, 0.3997, 0.6311, 0.1445]])\n",
      "tensor([[0.1617, 0.5476, 0.4154, 0.6731, 0.1571],\n",
      "        [0.8064, 0.8664, 0.0642, 0.0574, 0.3945]])\n",
      "tensor([[0.3032, 0.6162],\n",
      "        [0.0942, 0.3936],\n",
      "        [0.3536, 0.2133],\n",
      "        [0.9918, 0.1927],\n",
      "        [0.6711, 0.3129]])\n",
      "tensor([[1.1819, 1.2056, 1.0529, 1.1704, 0.6329],\n",
      "        [1.3089, 1.2697, 1.2616, 1.4713, 0.6159],\n",
      "        [1.0868, 1.1509, 0.9105, 1.0205, 0.5826],\n",
      "        [0.9633, 1.1880, 0.4865, 0.6706, 0.9861],\n",
      "        [2.1060, 2.0100, 1.5736, 1.8347, 1.2376]])\n",
      "tensor([[0.2558, 0.4919, 0.3648, 0.8303, 0.1401, 0.4583, 0.1443, 0.2191, 0.3032,\n",
      "         0.6162],\n",
      "        [0.2633, 0.1573, 0.1584, 0.9556, 0.5556, 0.2973, 0.8497, 0.0771, 0.0942,\n",
      "         0.3936],\n",
      "        [0.1617, 0.5476, 0.4154, 0.6731, 0.1571, 0.6465, 0.1874, 0.3082, 0.3536,\n",
      "         0.2133],\n",
      "        [0.8064, 0.8664, 0.0642, 0.0574, 0.3945, 0.7136, 0.7007, 0.9473, 0.9918,\n",
      "         0.1927],\n",
      "        [0.5852, 0.8945, 0.9340, 0.9404, 0.4619, 0.6428, 0.7178, 0.4280, 0.6711,\n",
      "         0.3129]])\n",
      "tensor([[5.2558, 5.4919, 5.3648, 5.8303, 5.1401],\n",
      "        [5.2633, 5.1573, 5.1584, 5.9556, 5.5556],\n",
      "        [5.1617, 5.5476, 5.4154, 5.6731, 5.1571],\n",
      "        [5.8064, 5.8664, 5.0642, 5.0574, 5.3945],\n",
      "        [5.5852, 5.8945, 5.9340, 5.9404, 5.4619]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5,5)\n",
    "b = torch.rand(5,5)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a+b) #Elementwise addition\n",
    "print(a*b) #Elementwise multiplication\n",
    "print(a[2:4,]) #Slice out rows\n",
    "print(b[:,3:5]) #Slice out columns\n",
    "print(torch.matmul(a,b)) #Matrix multiplication\n",
    "print(torch.cat((a,b),axis=1)) #Tensor concatenation\n",
    "\n",
    "#In place addition, notice the underscore and that the tensor has been permanently changed\n",
    "a.add_(5) \n",
    "print(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce94e83-8a46-4e2c-b113-8ed6cef54bfa",
   "metadata": {},
   "source": [
    "## 2. Datasets and DataLoaders\n",
    "Now, let's begin actually training a model.  For this tutorial, we will be training a neural network to predict the secondary structure of a protein given its evolutionary "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytDemo] *",
   "language": "python",
   "name": "conda-env-pytDemo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
