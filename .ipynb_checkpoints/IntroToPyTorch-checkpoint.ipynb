{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3608376-c567-4e35-8a8a-05621819b8f6",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "## 1. Tensors\n",
    "PyTorch operates through manipulation of a data structure known as the \"tensor\".  Tensors are mathematically known as a high-dimensional data structure (i.e., 3 or more dimensional matrices), but in PyTorch, any dimensionality of array can be represented as a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d063ec-0b22-4c08-8180-0b98a1c71cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#All are welcome in the world of PyTorch tensors!\n",
    "print(torch.ones(1)) #A scalar?\n",
    "print(torch.ones(10)) #A vector?\n",
    "print(torch.ones(3,3)) #A matrix?\n",
    "print(torch.ones(3,2,1)) #A 3D tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141180e-f9f7-4803-9e97-6424f462f6f5",
   "metadata": {},
   "source": [
    "Tensors must contain only entries of the same data type, and the dimension of each axis must be consistent across the layers of the tensor (i.e. a tensor is an *array* of *arrays* not a *list* of *lists* like Python likes to do).  If you're familiar with numpy arrays, Tensors can be easily constructed from python lists and numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560e5c7-bf98-4a5f-9281-06b252d440f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(torch.tensor([[i for i in range(10)] for j in range(10)]))\n",
    "print(torch.tensor(np.ones((10,10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490ea9-17f3-43b5-bbf8-405afb9909fa",
   "metadata": {},
   "source": [
    "You can easily create tensors of a given size using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cef09-3046-449e-b9d9-ddfd51647281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.zeros(3,3)) #Make a tensor of zeros\n",
    "print(torch.ones(3,3)) #Make a tensor of ones\n",
    "print(torch.rand(3,3)) #Make a tensor of random numbers between 0 and 1\n",
    "print(torch.empty(3,3)) #Make a tensor of uninitialized values (might be zeros, might be weird numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe42937-57c4-45d5-98fe-e5bd4716f891",
   "metadata": {},
   "source": [
    "A few attributes of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21de8ae-2daa-4e67-8791-5d0b7242160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5,5)\n",
    "\n",
    "print(a.size()) #Get the shape of the tensor\n",
    "print(a.dtype) #Get the data type of the entries\n",
    "print(a.device) #Get which device the tensor is on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c3790-e702-462d-b7db-db53cc4187f0",
   "metadata": {},
   "source": [
    "But hold on... what's that last one again?  What does it mean to have a tensor on a device?  The answer is that the main difference between numpy arrays and torch tensors is that tensors can be put on a GPU, thus allowing all mathematical operations involving it to be GPU accelerated!  For that to be able to happen, the following has to return true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01a211-3479-4054-b5b5-a34fb210dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f8448-8de9-44e9-92d0-71457e8b8a44",
   "metadata": {},
   "source": [
    "If this is true, congratulations!  You're ready to accelerate using the GPU!  If it's false, you either don't have a GPU, or don't have CUDA or PyTorch set up in a way that allows for GPU acceleration.  Getting this set up is incredibly system specific and beyond the scope of this tutorial.  Regardless, if you have access to the GPU, you can take advantage of it by moving your tensor to the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d310f5-04d9-430f-8cf3-19153dd56b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    a.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1097f51-ae29-46fe-9638-eb31ca67d40d",
   "metadata": {},
   "source": [
    "Note that all created tensors will be stored on the CPU by default unless specified otherwise on initialization (i.e. specifying `device=\"cuda\"` when the tensor is made).  Also, operations between tensors require that the tensors be on the same device; you'll get an error otherwise.  So what are the operations can you do to tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430f621-7831-4efd-9dbf-deec2123da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5,5)\n",
    "b = torch.rand(5,5)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a+b) #Elementwise addition\n",
    "print(a*b) #Elementwise multiplication\n",
    "print(a[2:4,:]) #Slice out rows\n",
    "print(b[:,3:5]) #Slice out columns\n",
    "print(torch.matmul(a,b)) #Matrix multiplication\n",
    "print(torch.cat((a,b),axis=1)) #Tensor concatenation\n",
    "\n",
    "#In place addition, notice the underscore and that the tensor has been permanently changed\n",
    "a.add_(5) \n",
    "print(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce94e83-8a46-4e2c-b113-8ed6cef54bfa",
   "metadata": {},
   "source": [
    "## 2. Datasets and DataLoaders\n",
    "Now, let's begin actually training a model.  For this tutorial, we will be training a neural network to predict the secondary structure of a protein given its amino acid sequence (spoiler alert, it's not going to be a very well-performing predictor but it's good enough to show off how PyTorch works).  We first have a data file that has 100 proteins, with the amino acid sequence on one line and the corresponding secondary structure sequence on the next line (both of these were extracted from using Rosetta simple metrics).  How does one prep this data for use in PyTorch?  The answer is to define a **Dataset**, which is a user-defined class that encapsulates the input data.  The dataset must have two functions to be valid: a function which returns a single data point (both input features and label) given an index, and a function which returns the number of points in the whole dataset.  Here is an example dataset for our specific problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62718719-1bee-44ab-a554-213f8193fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SSData(Dataset):\n",
    "\n",
    "    #Class constructor, typical OOP stuff\n",
    "    def __init__(self, ssfasta, windowPadding=7):\n",
    "        self.ssData = self.readFasta(ssfasta, windowPadding)\n",
    "\n",
    "    #Returns the length of the whole dataset\n",
    "    def __len__(self):\n",
    "        return len(self.ssData)\n",
    "\n",
    "    #Returns a (featureTensor, label) tuple given an index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ssData[idx]\n",
    "\n",
    "    #Reads in the input file, one hot encodes the sequences, and chops them up into chunks\n",
    "    def readFasta(self, ssfasta, windowPadding):\n",
    "        AAalphabet = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        SSalphabet = \"LEH\"\n",
    "        \n",
    "        try:\n",
    "            f = open(ssfasta)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Input fasta not found!\")\n",
    "            return []\n",
    "\n",
    "        ssData = []\n",
    "        for line in f:\n",
    "            SSseq = f.readline()\n",
    "            AAseq = f.readline()\n",
    "            if len(AAseq) != len(SSseq):\n",
    "                print(\"Check input data! Seq lengths are not the same...\")\n",
    "                continue\n",
    "\n",
    "            #We're ignoring the ends of the sequence; that's fine for this application\n",
    "            for i in range(windowPadding, len(AAseq)-windowPadding-1):\n",
    "                AAencoding = torch.zeros(2*windowPadding+1, len(AAalphabet))\n",
    "                SSencoding = torch.zeros(len(SSalphabet))\n",
    "                AAchunk = AAseq[i-windowPadding:i+windowPadding+1]\n",
    "                for j in range(len(AAchunk)):\n",
    "                    AAencoding[j, AAalphabet.index(AAchunk[j])] = 1.\n",
    "                SSencoding[SSalphabet.index(SSseq[i])] = 1.\n",
    "                ssData.append((AAencoding, SSencoding))\n",
    "        return ssData\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035ca21-0a91-4e70-be11-f5ed021595ff",
   "metadata": {},
   "source": [
    "Now all we need to do to load in the data is to construct an `SSData` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27894958-250d-4e12-87e8-e110af6fc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SSData(\"ssData.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f694b1-273b-4cc9-96c8-af826745d699",
   "metadata": {},
   "source": [
    "So you could simply just loop through the dataset and pull out every data point for training, but this is insufficient for most real ML applications.  For most training schemes, you need to at least shuffle and batch the data.  Thankfully, PyTorch offers a tool called the **DataLoader** which will handle these tasks for you; all you have to do is iterate through the dataloader instead of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722f5ff-69ad-4684-bc80-387e544ee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Pro tip: if you're going to be using the GPU, pin_memory=True in the DataLoader can make things more efficient\n",
    "dl = DataLoader(ds, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8decc-6366-44d2-9971-df6b58c0f7be",
   "metadata": {},
   "source": [
    "## 3. Defining the Network\n",
    "Ok, now that we've got all our data, we're ready to train! Or... maybe not because there's still no network to train.  Let's fix that by defining another class which defines the neural network.  This class will have a constructor, which defines the architecture, and a \"forward\" function which describes the flow of the input through the network to its eventual output.  Most of the definitions here are facilitated by the `torch.nn` module, which has predefined layers, functions, etc. for most generic NN applications.  Here we define a simple fully connected neural network with one hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454e9c6-13ca-453c-a912-38ca6ecbcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, hiddenNodes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        #nn.Sequential lets you define a module of actions rather than calling them one by one\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.Linear(300, hiddenNodes), #Input vector to hidden layer connections\n",
    "            nn.ReLU(), #Hidden layer activation\n",
    "            nn.Linear(hiddenNodes, 3), #Hidden layer to output layer connections\n",
    "            nn.Sigmoid() #Output layer activation\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #DON'T flatten the first (0 index) dimension; that's the batch dimension\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fcnn(x)\n",
    "        return x\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b39662-c40d-4d17-8d5c-981dcae8abb0",
   "metadata": {},
   "source": [
    "Clearly there's way more to writing complicated neural network architectures than what's presented here (normalizations, convolutions, dropout, pooling, etc.), but all of them have implementations in `torch.nn` that are easy to snap together in a framework such as this.  The most complicated thing about building a network usually ends up being ensuring that the dimensions agree between layers as opposed to actually implementing the layers themselves.  Here's a more deep learning-based network (i.e., CNN) example just to show off what `torch.nn` can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e2931-6d23-439f-b235-15198bd90063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #nn.Sequential lets you define a module of actions rather than calling them one by one\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(20,8,3),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(8,2,3),\n",
    "            nn.BatchNorm1d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(8)\n",
    "        )\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = torch.transpose(x, 1, 2) #Conv1d wants channels as the middle dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fcnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2eed0-5beb-4bc8-9d39-680657b7d217",
   "metadata": {},
   "source": [
    "## 4. Training the network!\n",
    "Now we're finally ready to train the network.  Let's first start by instantiating an object of the network defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2a8c6-909b-4b55-8403-62c7674b57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FCNN()\n",
    "if torch.cuda.is_available():\n",
    "    net = net.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a732c-6918-4a13-9780-a15facf20568",
   "metadata": {},
   "source": [
    "Next, we need to select the loss function and optimizer.  Both of these will be pre-defined in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7206dd-af80-4bca-a560-2b876ffb0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\") #Cross entropy loss for a multi-class classification problem\n",
    "optimizer = optim.Adam(net.parameters()) #Adam optimization on the parameters of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf0fa1-4f29-4201-9b09-72ffdb4d7b00",
   "metadata": {},
   "source": [
    "Before we begin to define the training epochs, let's talk about how PyTorch handles backpropagation despite having such a wide diversity of architectures and functionalities.  The secret sauce to how this versatility works is Autograd, a package which allows for differentiation given arbitrary functions.  Whenever you do any operation on a tensor with a `requires_grad=True` property, Autograd creates a graph-based history of those operations, then retraces that graph to calculate the total gradient when `backward()` is called.  However, calling `backward()` does NOT reset the gradient, therefore `zero_grad()` must be called before putting batches through the network.  With that context, we're now ready to define the training epoch loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dab2e7-c1ae-43c7-9cc8-1908d2c7c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses = []\n",
    "while epoch < 100:\n",
    "    totalLoss = 0.\n",
    "    for features, labels in dl:\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalLoss += loss.item()\n",
    "    epoch += 1\n",
    "    losses.append(totalLoss)\n",
    "    print (\"Epoch %d Loss: %.3f\"%(epoch,totalLoss), end=\"\\r\")\n",
    "\n",
    "#Matplotlib code to plot the loss vs. epochs\n",
    "plt.plot([i for i in range(len(losses))],losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b22c9-8f7e-4c4e-8097-8d74a6a8efa6",
   "metadata": {},
   "source": [
    "Considering the network is really simple and the dataset isn't that big, training should only take a minute or two, even without the GPU acceleration.  You should see the loss steadily decrease as the epochs go on; this shows that the network is definitely learning.  However, the job is not over because training loss is not a suitable performance metric.  Instead, we need to evaluate the classification performance on the network on something that's NOT in the training set.\n",
    "\n",
    "## 5. Validation and Benchmarking\n",
    "There are two main philosophies for measuring validation performance: create a validation set by random split (reserved for when training is incredibly lengthy) or k-fold cross validation.  We're going to use the latter, although the former can be solved by using `torch.utils.data.random_split()`.  First, we'll adapt the training loop above into a contained function.  We'll also add in a validation performance calculation for every epoch using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78026e-42c1-4aa0-8e79-8c9b471850c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def trainModel(trainLoader, validLoader, arch=FCNN):\n",
    "    net = arch()\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.to(\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") #Cross entropy loss for a multi-class classification problem\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    perfs = []\n",
    "    while epoch < 100:\n",
    "        totalLoss = 0.\n",
    "        for features, labels in trainLoader:\n",
    "            if torch.cuda.is_available():\n",
    "                features = features.to(\"cuda\")\n",
    "                labels = labels.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            totalLoss += loss.item()\n",
    "        epoch += 1\n",
    "        losses.append(totalLoss)\n",
    "        #Here's the validation calculation, torch.no_grad() ensures Autograd is off\n",
    "        with torch.no_grad():\n",
    "            net.eval() #This changes how some kind of layers (e.g. dropout) behave so that prediction is done correctly\n",
    "            truths = torch.empty(0)\n",
    "            preds = torch.empty(0)\n",
    "            if torch.cuda.is_available():\n",
    "                truths = truths.to(\"cuda\")\n",
    "                preds = preds.to(\"cuda\")\n",
    "            for features, labels in validLoader:\n",
    "                if torch.cuda.is_available():\n",
    "                    features = features.to(\"cuda\")\n",
    "                    labels = labels.to(\"cuda\")\n",
    "                outputs = torch.argmax(net(features), dim=1)\n",
    "                preds = torch.cat((preds,outputs))\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "                truths = torch.cat((truths,labels))\n",
    "\n",
    "            preds = preds.numpy()\n",
    "            truths = truths.numpy()\n",
    "            perf = f1_score(truths, preds, average=\"macro\")\n",
    "            perfs.append(perf)\n",
    "            net.train() #Set the model back to training mode\n",
    "\n",
    "        print (\"Epoch %d Loss: %.3f Validation F1: %.3f\"%(epoch,totalLoss,perfs[-1]), end=\"\\r\")\n",
    "\n",
    "    #Matplotlib code, you can uncomment the loss plots if you really want them\n",
    "    #plt.plot([i for i in range(len(losses))],losses)\n",
    "    #plt.xlabel(\"Epochs\")\n",
    "    #plt.ylabel(\"Loss\")\n",
    "    #plt.show()\n",
    "    plt.plot([i for i in range(len(perfs))],perfs)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da04c1-6734-4187-a1c4-fd9924ec0c42",
   "metadata": {},
   "source": [
    "PyTorch can create a subset from a dataset by passing a set of indices.  If we use scikit-learn, we can easily create a set of indices for each fold of a cross validation benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc926426-d3b3-4399-8355-4fb7abcf7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def crossValidation(dataset):\n",
    "    splits = KFold(n_splits=5, shuffle=True)\n",
    "    for i, (trainIdx, validIdx) in enumerate(splits.split(range(len(dataset)))):\n",
    "        print(\"FOLD %d\"%(i+1))\n",
    "        trainSet = Subset(dataset, trainIdx)\n",
    "        validSet = Subset(dataset, validIdx)\n",
    "        trainLoader = DataLoader(trainSet, batch_size=64, shuffle=True)\n",
    "        validLoader = DataLoader(validSet, batch_size=64, shuffle=True)\n",
    "        trainModel(trainLoader, validLoader, arch=FCNN)\n",
    "\n",
    "crossValidation(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eca9bf-6277-4749-bf15-e0a7da96b7cc",
   "metadata": {},
   "source": [
    "If your runs are anything like mine, clearly the model is mediocre in performance and potentially even overfitting.  This is likely because of the incredibly uncomplicated input feature (one-hot encoding does not provide a lot of information to the classifier).  However, this toy example still serves its purpose, which is to show the basics of PyTorch in action.  Feel free to use some of the deeper neural network architectures and to play with various parameters of the networks (activation functions, node counts, etc.).  Perhaps with enough neural network wizardry, the performance will improve..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytDemo] *",
   "language": "python",
   "name": "conda-env-pytDemo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
